{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start due to the missing module '_sqlite3'. Consider installing this module.\n",
      "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresMissingModule'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import yaml\n",
    "import numpy as np\n",
    "import pytorch_lightning\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch_geometric.loader import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model to plot\n",
    "model_dir = \"/home/rxm7244/Projects/CoSFan-NeuralSurrogate/experiments/feedforwardmask_synthetic_continual_er_1111_1.0/feedforwardmask/version_0\"\n",
    "model = model_dir.split(\"/\")[3]\n",
    "memory = model_dir.split(\"/\")[2].split(\"_\")[-1]\n",
    "cfg_dir = f\"{model_dir}/task_0/hparams.yaml\"\n",
    "\n",
    "with open(cfg_dir, 'r') as stream:\n",
    "    cfg = yaml.safe_load(stream)\n",
    "print(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over task ids and stack datasets\n",
    "npzfile = np.load(f\"../data/synthetic/scar_15/train.npz\")\n",
    "\n",
    "# Load in data sources\n",
    "xs = npzfile['xs']\n",
    "xs = np.swapaxes(xs, 2, 1)\n",
    "\n",
    "xs = xs[:, :, :60]\n",
    "xs = xs[:, :, 5:]\n",
    "\n",
    "labels = npzfile['label'].astype(int)\n",
    "scar = npzfile['scar']\n",
    "names = npzfile['heart_name']\n",
    "print(xs.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs_gt = np.load(\"/home/rxm7244/Projects/CoSFan-NeuralSurrogate/experiments/feedforwardmask_synthetic_continual_er_1111_1.0/feedforwardmask/version_0/task_8/test_8_train/test_8_train_signals.npy\", allow_pickle=True)\n",
    "xs_pred = np.load(\"/home/rxm7244/Projects/CoSFan-NeuralSurrogate/experiments/feedforwardmask_synthetic_continual_er_1111_1.0/feedforwardmask/version_0/task_8/test_8_train/test_8_train_preds.npy\", allow_pickle=True)\n",
    "print(xs_gt.shape, xs_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "\n",
    "class HeartEmptyGraphDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A dataset of Data objects (in pytorch geometric) with graph attributes\n",
    "    from a pre-defined graph hierarchy. The features and target values are \n",
    "    set to zeros in given graph.\n",
    "    Not suitable for training.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 mesh_graph,\n",
    "                 label_type=None):\n",
    "        self.graph = mesh_graph\n",
    "        dim = self.graph.pos.shape[0]\n",
    "        self.datax = np.zeros((dim, 101))\n",
    "        self.label = np.zeros((101))\n",
    "\n",
    "    def __len__(self):\n",
    "        return (self.datax.shape[1])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.from_numpy(self.datax[:, [idx]]).float()  # torch.tensor(dataset[:,[i]],dtype=torch.float)\n",
    "        y = torch.from_numpy(self.label[[idx]]).float()  # torch.tensor(label_aha[[i]],dtype=torch.float)\n",
    "\n",
    "        sample = Data(x=x,\n",
    "                      y=y,\n",
    "                      edge_index=self.graph.edge_index,\n",
    "                      edge_attr=self.graph.edge_attr,\n",
    "                      pos=self.graph.pos)\n",
    "        return sample\n",
    "\n",
    "\n",
    "def load_graph(filename, load_torso=0, graph_method=None):\n",
    "    with open(filename + '.pickle', 'rb') as f:\n",
    "        g = pickle.load(f)\n",
    "        g1 = pickle.load(f)\n",
    "        g2 = pickle.load(f)\n",
    "        g3 = pickle.load(f)\n",
    "        g4 = pickle.load(f)\n",
    "\n",
    "        P10 = pickle.load(f)\n",
    "        P21 = pickle.load(f)\n",
    "        P32 = pickle.load(f)\n",
    "        P43 = pickle.load(f)\n",
    "\n",
    "        if load_torso == 1:\n",
    "            t_g = pickle.load(f)\n",
    "            t_g1 = pickle.load(f)\n",
    "            t_g2 = pickle.load(f)\n",
    "            t_g3 = pickle.load(f)\n",
    "\n",
    "            t_P10 = pickle.load(f)\n",
    "            t_P21 = pickle.load(f)\n",
    "            t_P32 = pickle.load(f)\n",
    "\n",
    "            if graph_method == 'bipartite':\n",
    "                Hs = pickle.load(f)\n",
    "                Ps = pickle.load(f)\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "\n",
    "    if load_torso == 0:\n",
    "        P01 = P10 / P10.sum(axis=0)\n",
    "        P12 = P21 / P21.sum(axis=0)\n",
    "        P23 = P32 / P32.sum(axis=0)\n",
    "        P34 = P43 / P43.sum(axis=0)\n",
    "\n",
    "        P01 = torch.from_numpy(np.transpose(P01)).float()\n",
    "        P12 = torch.from_numpy(np.transpose(P12)).float()\n",
    "        P23 = torch.from_numpy(np.transpose(P23)).float()\n",
    "        P34 = torch.from_numpy(np.transpose(P34)).float()\n",
    "\n",
    "        P10 = torch.from_numpy(P10).float()\n",
    "        P21 = torch.from_numpy(P21).float()\n",
    "        P32 = torch.from_numpy(P32).float()\n",
    "        P43 = torch.from_numpy(P43).float()\n",
    "\n",
    "        return g, g1, g2, g3, g4, P10, P21, P32, P43, P01, P12, P23, P34\n",
    "    elif load_torso == 1:\n",
    "        t_P01 = t_P10 / t_P10.sum(axis=0)\n",
    "        t_P12 = t_P21 / t_P21.sum(axis=0)\n",
    "        t_P23 = t_P32 / t_P32.sum(axis=0)\n",
    "\n",
    "        t_P01 = torch.from_numpy(np.transpose(t_P01)).float()\n",
    "        t_P12 = torch.from_numpy(np.transpose(t_P12)).float()\n",
    "        t_P23 = torch.from_numpy(np.transpose(t_P23)).float()\n",
    "\n",
    "        if graph_method == 'bipartite':\n",
    "            Ps = torch.from_numpy(Ps).float()\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        P10 = torch.from_numpy(P10).float()\n",
    "        P21 = torch.from_numpy(P21).float()\n",
    "        P32 = torch.from_numpy(P32).float()\n",
    "        P43 = torch.from_numpy(P43).float()\n",
    "\n",
    "        return g, g1, g2, g3, g4, P10, P21, P32, P43,\\\n",
    "            t_g, t_g1, t_g2, t_g3, t_P01, t_P12, t_P23, Hs, Ps\n",
    "\n",
    "\n",
    "def get_params(data_path, heart_name, device, batch_size, load_torso=0, load_physics=0, graph_method=None):\n",
    "    if load_physics == 1:\n",
    "        # Load physics parameters\n",
    "        physics_name = heart_name.split('_')[0]\n",
    "        physics_dir = os.path.join(data_path, 'physics/{}/'.format(physics_name))\n",
    "        mat_files = scipy.io.loadmat(os.path.join(physics_dir, 'h_L.mat'), squeeze_me=True, struct_as_record=False)\n",
    "        L = mat_files['h_L']\n",
    "\n",
    "        mat_files = scipy.io.loadmat(os.path.join(physics_dir, 'H.mat'), squeeze_me=True, struct_as_record=False)\n",
    "        H = mat_files['H']\n",
    "\n",
    "        L = torch.from_numpy(L).float().to(device)\n",
    "        print('Load Laplacian: {} x {}'.format(L.shape[0], L.shape[1]))\n",
    "\n",
    "        H = torch.from_numpy(H).float().to(device)\n",
    "        print('Load H matrix: {} x {}'.format(H.shape[0], H.shape[1]))\n",
    "\n",
    "    # Load geometrical parameters\n",
    "    graph_file = os.path.join(data_path, 'signal/{}/{}_{}'.format(heart_name, heart_name, graph_method))\n",
    "    if load_torso == 0:\n",
    "        g, g1, g2, g3, g4, P10, P21, P32, P43, P01, P12, P23, P34 = \\\n",
    "            load_graph(graph_file, load_torso, graph_method)\n",
    "    else:\n",
    "        g, g1, g2, g3, g4, P10, P21, P32, P43,\\\n",
    "        t_g, t_g1, t_g2, t_g3, t_P01, t_P12, t_P23, Hs, Ps = load_graph(graph_file, load_torso, graph_method)\n",
    "\n",
    "    num_nodes = [g.pos.shape[0], g1.pos.shape[0], g2.pos.shape[0], g3.pos.shape[0],\n",
    "                 g4.pos.shape[0]]\n",
    "    # print('number of nodes:', num_nodes)\n",
    "\n",
    "    g_dataset = HeartEmptyGraphDataset(mesh_graph=g)\n",
    "    g_loader = DataLoader(g_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    bg = next(iter(g_loader))\n",
    "\n",
    "    g1_dataset = HeartEmptyGraphDataset(mesh_graph=g1)\n",
    "    g1_loader = DataLoader(g1_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    bg1 = next(iter(g1_loader))\n",
    "\n",
    "    g2_dataset = HeartEmptyGraphDataset(mesh_graph=g2)\n",
    "    g2_loader = DataLoader(g2_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    bg2 = next(iter(g2_loader))\n",
    "\n",
    "    g3_dataset = HeartEmptyGraphDataset(mesh_graph=g3)\n",
    "    g3_loader = DataLoader(g3_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    bg3 = next(iter(g3_loader))\n",
    "\n",
    "    g4_dataset = HeartEmptyGraphDataset(mesh_graph=g4)\n",
    "    g4_loader = DataLoader(g4_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    bg4 = next(iter(g4_loader))\n",
    "\n",
    "    P10 = P10.to(device)\n",
    "    P21 = P21.to(device)\n",
    "    P32 = P32.to(device)\n",
    "    P43 = P43.to(device)\n",
    "\n",
    "    bg1 = bg1.to(device)\n",
    "    bg2 = bg2.to(device)\n",
    "    bg3 = bg3.to(device)\n",
    "    bg4 = bg4.to(device)\n",
    "\n",
    "    bg = bg.to(device)\n",
    "\n",
    "    if load_torso == 0:\n",
    "        P01 = P01.to(device)\n",
    "        P12 = P12.to(device)\n",
    "        P23 = P23.to(device)\n",
    "        P34 = P34.to(device)\n",
    "\n",
    "        P1n = np.ones((num_nodes[1], 1))\n",
    "        Pn1 = P1n / P1n.sum(axis=0)\n",
    "        Pn1 = torch.from_numpy(np.transpose(Pn1)).float()\n",
    "        P1n = torch.from_numpy(P1n).float()\n",
    "        P1n = P1n.to(device)\n",
    "        Pn1 = Pn1.to(device)\n",
    "\n",
    "        params = {\n",
    "            \"bg1\": bg1, \"bg2\": bg2, \"bg3\": bg3, \"bg4\": bg4,\n",
    "            \"P01\": P01, \"P12\": P12, \"P23\": P23, \"P34\": P34,\n",
    "            \"P10\": P10, \"P21\": P21, \"P32\": P32, \"P43\": P43,\n",
    "            \"P1n\": P1n, \"Pn1\": Pn1, \"num_nodes\": num_nodes, \"g\": g, \"bg\": bg\n",
    "        }\n",
    "    elif load_torso == 1:\n",
    "        t_num_nodes = [t_g.pos.shape[0], t_g1.pos.shape[0], t_g2.pos.shape[0], t_g3.pos.shape[0]]\n",
    "        print('number of nodes on torso:', t_num_nodes)\n",
    "        t_g_dataset = HeartEmptyGraphDataset(mesh_graph=t_g)\n",
    "        t_g_loader = DataLoader(t_g_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "        t_bg = next(iter(t_g_loader))\n",
    "\n",
    "        t_g1_dataset = HeartEmptyGraphDataset(mesh_graph=t_g1)\n",
    "        t_g1_loader = DataLoader(t_g1_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "        t_bg1 = next(iter(t_g1_loader))\n",
    "\n",
    "        t_g2_dataset = HeartEmptyGraphDataset(mesh_graph=t_g2)\n",
    "        t_g2_loader = DataLoader(t_g2_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "        t_bg2 = next(iter(t_g2_loader))\n",
    "\n",
    "        t_g3_dataset = HeartEmptyGraphDataset(mesh_graph=t_g3)\n",
    "        t_g3_loader = DataLoader(t_g3_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "        t_bg3 = next(iter(t_g3_loader))\n",
    "\n",
    "        t_P01 = t_P01.to(device)\n",
    "        t_P12 = t_P12.to(device)\n",
    "        t_P23 = t_P23.to(device)\n",
    "\n",
    "        t_bg1 = t_bg1.to(device)\n",
    "        t_bg2 = t_bg2.to(device)\n",
    "        t_bg3 = t_bg3.to(device)\n",
    "        t_bg = t_bg.to(device)\n",
    "\n",
    "        if graph_method == 'bipartite':\n",
    "            H_dataset = HeartEmptyGraphDataset(mesh_graph=Hs)\n",
    "            H_loader = DataLoader(H_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "            H_inv = next(iter(H_loader))\n",
    "\n",
    "            H_inv = H_inv.to(device)\n",
    "            Ps = Ps.to(device)\n",
    "\n",
    "            if load_physics == 1:\n",
    "                params = {\n",
    "                    \"bg1\": bg1, \"bg2\": bg2, \"bg3\": bg3, \"bg4\": bg4,\n",
    "                    \"P10\": P10, \"P21\": P21, \"P32\": P32, \"P43\": P43,\n",
    "                    \"num_nodes\": num_nodes, \"g\": g, \"bg\": bg,\n",
    "                    \"t_bg1\": t_bg1, \"t_bg2\": t_bg2, \"t_bg3\": t_bg3,\n",
    "                    \"t_P01\": t_P01, \"t_P12\": t_P12, \"t_P23\": t_P23,\n",
    "                    \"t_num_nodes\": t_num_nodes, \"t_g\": t_g, \"t_bg\": t_bg,\n",
    "                    \"H_inv\": H_inv, \"P\": Ps,\n",
    "                    \"H\": H, \"L\": L\n",
    "                }\n",
    "            else:\n",
    "                params = {\n",
    "                    \"bg1\": bg1, \"bg2\": bg2, \"bg3\": bg3, \"bg4\": bg4,\n",
    "                    \"P10\": P10, \"P21\": P21, \"P32\": P32, \"P43\": P43,\n",
    "                    \"num_nodes\": num_nodes, \"g\": g, \"bg\": bg,\n",
    "                    \"t_bg1\": t_bg1, \"t_bg2\": t_bg2, \"t_bg3\": t_bg3,\n",
    "                    \"t_P01\": t_P01, \"t_P12\": t_P12, \"t_P23\": t_P23,\n",
    "                    \"t_num_nodes\": t_num_nodes, \"t_g\": t_g, \"t_bg\": t_bg,\n",
    "                    \"H_inv\": H_inv, \"P\": Ps\n",
    "                }\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    return params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.colors as colors\n",
    "from mpl_toolkits.mplot3d.art3d import Poly3DCollection\n",
    "\n",
    "def create_mesh_faces(edge_index):\n",
    "    \"\"\"Create triangular faces from edge connections\"\"\"\n",
    "    # Convert edge indices to set of edges for faster lookup\n",
    "    edges = set()\n",
    "    for i in range(edge_index.shape[1]):\n",
    "        v1, v2 = sorted([int(edge_index[0,i]), int(edge_index[1,i])])\n",
    "        edges.add((v1, v2))\n",
    "    \n",
    "    # Find triangles by checking edge connections\n",
    "    faces = []\n",
    "    for e1 in edges:\n",
    "        for v3 in range(edge_index.max().item() + 1):\n",
    "            # Check if v3 forms a triangle with edge e1\n",
    "            e2 = tuple(sorted([e1[1], v3]))\n",
    "            e3 = tuple(sorted([e1[0], v3]))\n",
    "            if e2 in edges and e3 in edges:\n",
    "                face = tuple(sorted([e1[0], e1[1], v3]))\n",
    "                faces.append(face)\n",
    "    \n",
    "    return np.array(list(set(faces)))\n",
    "\n",
    "\n",
    "def visualize_potential_comparison(data, params, timeframes=[0, 25, 50, 75], figsize=(20, 12)):\n",
    "    \"\"\"\n",
    "    Visualize ground truth and multiple predictions in separate rows with MSE values in titles\n",
    "    \"\"\"\n",
    "    # Get positions and edge indices from params\n",
    "    positions = params['g'].pos.numpy()\n",
    "    edge_index = params['g'].edge_index.numpy()\n",
    "    faces = create_mesh_faces(edge_index)\n",
    "    \n",
    "    gt_data = data.pop(\"GT\")  # Remove GT from data dict to handle separately\n",
    "    n_rows = len(data) + 1  # GT + number of baselines\n",
    "    n_views = 2  # Top and bottom views\n",
    "    \n",
    "    # Create figure with subplots and space for colorbar\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    gs = plt.GridSpec(n_rows + 1, len(timeframes)*n_views + 1,\n",
    "                     height_ratios=[1]*n_rows + [0.15],\n",
    "                     width_ratios=[0.15] + [1]*(len(timeframes)*n_views),\n",
    "                     hspace=0.2,\n",
    "                     wspace=0.05)\n",
    "\n",
    "    # Create custom colormap with nonlinear color transitions\n",
    "    potential_cmap = plt.cm.gist_rainbow\n",
    "    \n",
    "    # Replace the existing normalization with SymLogNorm\n",
    "    all_potentials = np.concatenate([gt_data] + list(data.values()), axis=1)\n",
    "    vmin, vmax = np.min(all_potentials), np.max(all_potentials)\n",
    "    abs_max = max(abs(vmin), abs(vmax))\n",
    "    # Define the linear threshold - values smaller than this will be scaled logarithmically\n",
    "    linthresh = abs_max / 100  # Adjust this value to control the transition point\n",
    "    potential_norm = colors.SymLogNorm(linthresh=linthresh, vmin=-abs_max, vmax=abs_max)\n",
    "\n",
    "    \n",
    "    # Calculate MSE per timeframe for each baseline\n",
    "    mse_values = {}\n",
    "    for method in data.keys():\n",
    "        mse_values[method] = np.zeros(len(timeframes))\n",
    "        for i, t in enumerate(timeframes):\n",
    "            mse_values[method][i] = np.mean((gt_data[:, t] - data[method][:, t])**2)\n",
    "    \n",
    "    # Views configuration\n",
    "    views = [\n",
    "        {'elev': 90, 'azim': 0},  # Top-down view\n",
    "        {'elev': -90, 'azim': 0}  # Bottom-up view\n",
    "    ]\n",
    "    \n",
    "    # Create ordered list of rows starting with GT\n",
    "    rows = [(\"GT\", gt_data)] + [(method, pred) for method, pred in data.items()]\n",
    "    \n",
    "    for row_idx, (label, potentials) in enumerate(rows):\n",
    "        # Add row label\n",
    "        label_ax = fig.add_subplot(gs[row_idx, 0])\n",
    "        label_ax.text(-1.25, 0.5, label, rotation=0,\n",
    "                     horizontalalignment='center', verticalalignment='center',\n",
    "                     fontsize=16, fontdict={'fontweight': 'bold'})\n",
    "        label_ax.axis('off')\n",
    "        \n",
    "        for t_idx, t in enumerate(timeframes):\n",
    "            for v_idx, view in enumerate(views):\n",
    "                col_idx = t_idx * n_views + v_idx + 1\n",
    "                \n",
    "                ax = fig.add_subplot(gs[row_idx, col_idx], projection='3d')\n",
    "                \n",
    "                # Create mesh with current timeframe data\n",
    "                mesh_vertices = positions[faces]\n",
    "                face_colors = np.mean(potentials[:, t][faces], axis=1)\n",
    "                \n",
    "                mesh = Poly3DCollection(mesh_vertices, \n",
    "                                      facecolors=potential_cmap(potential_norm(face_colors)))\n",
    "                mesh.set_edgecolor('black')\n",
    "                mesh.set_linewidth(0.1)\n",
    "                ax.add_collection3d(mesh)\n",
    "        \n",
    "                # Set axis limits and scaling\n",
    "                x_range = positions[:, 0].max() - positions[:, 0].min()\n",
    "                y_range = positions[:, 1].max() - positions[:, 1].min()\n",
    "                z_range = positions[:, 2].max() - positions[:, 2].min()\n",
    "                \n",
    "                x_center = (positions[:, 0].max() + positions[:, 0].min()) / 2\n",
    "                y_center = (positions[:, 1].max() + positions[:, 1].min()) / 2\n",
    "                z_center = (positions[:, 2].max() + positions[:, 2].min()) / 2\n",
    "                \n",
    "                scale_factor = 0.35\n",
    "                ax.set_xlim(x_center - x_range*scale_factor, x_center + x_range*scale_factor)\n",
    "                ax.set_ylim(y_center - y_range*scale_factor, y_center + y_range*scale_factor)\n",
    "                ax.set_zlim(z_center - z_range*scale_factor, z_center + z_range*scale_factor)\n",
    "                \n",
    "                ax.set_box_aspect([1, 1, 1])\n",
    "                ax.set_axis_off()\n",
    "                ax.grid(False)\n",
    "                \n",
    "                ax.view_init(elev=view['elev'], azim=view['azim'])\n",
    "                \n",
    "                # Set titles only on first view of each pair\n",
    "                if v_idx == 0:\n",
    "                    if row_idx == 0:  # Ground Truth row\n",
    "                        ax.set_title(f'Time: {t + 5}', pad=5, x=1, fontsize=12, \n",
    "                                   fontdict={'fontweight': 'bold'})\n",
    "                    else:  # Baseline rows\n",
    "                        method = label\n",
    "                        ax.set_title(f'MSE: {mse_values[method][t_idx]:.2e}', \n",
    "                                   pad=5, x=1, fontsize=10,\n",
    "                                   fontdict={'fontweight': 'bold'})\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(bottom=0.1, hspace=0.4, wspace=0.1)\n",
    "    \n",
    "    # Add separator lines between timeframe pairs\n",
    "    for t_idx in range(len(timeframes)-1):\n",
    "        # Get position of current pair's second view\n",
    "        current_ax = fig.axes[1 + t_idx*2]  # Skip label axes and get second view\n",
    "        next_ax = fig.axes[2 + (t_idx+1)*2]  # Get first view of next pair\n",
    "        \n",
    "        # Calculate separator position\n",
    "        current_pos = current_ax.get_position()\n",
    "        next_pos = next_ax.get_position()\n",
    "        sep_x = (current_pos.x1 + next_pos.x0) / 2\n",
    "        \n",
    "        # Add separator line for both rows\n",
    "        for row in range(2):\n",
    "            ax_sep = fig.add_axes([sep_x - 0.001, 0.1, 0.002, 0.8])  # [x, y, width, height]\n",
    "            ax_sep.axvspan(0, 1, color='black')\n",
    "            ax_sep.axis('off')\n",
    "    \n",
    "    # Add colorbar\n",
    "    sm_potential = plt.cm.ScalarMappable(cmap=potential_cmap, norm=potential_norm)\n",
    "    cbar_ax = fig.add_subplot(gs[-1, 1:])\n",
    "    cbar = plt.colorbar(sm_potential, cax=cbar_ax, orientation='horizontal')\n",
    "    cbar_ax.set_title('Potential', y=0, x=-0.01, rotation=0, ha='right', va='center', fontweight='bold')\n",
    "    \n",
    "    plt.savefig(\"ClusterMeshFigure.png\", dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Choose base task\n",
    "seed = 3333\n",
    "idx = 11\n",
    "base_task = 8\n",
    "test_task = 4\n",
    "pred_path = f\"task_{base_task}/test_{test_task}_train/test_{test_task}_train_preds.npy\"\n",
    "\n",
    "# Data samples to use\n",
    "data = {\n",
    "    \"GT\": np.load(f\"/home/rxm7244/Projects/CoSFan-NeuralSurrogate/experiments/feedforwardmask_synthetic_continual_er_{seed}_1.0/feedforwardmask/version_0/task_{base_task}/test_{test_task}_train/test_{test_task}_train_signals.npy\", allow_pickle=True)[idx],\n",
    "    \n",
    "    # \"C-ER\": np.load(f\"/home/rxm7244/Projects/CoSFan-NeuralSurrogate/experiments/feedforwardmask_synthetic_continual_er_{seed}_1.0/feedforwardmask/version_0/{pred_path}\", allow_pickle=True)[idx],\n",
    "    # \"C-RS\": np.load(f\"/home/rxm7244/Projects/CoSFan-NeuralSurrogate/experiments/feedforwardmask_synthetic_continual_task_aware_{seed}_1.0/feedforwardmask/version_0/{pred_path}\", allow_pickle=True)[idx],\n",
    "    # \"C-NL\": np.load(f\"/home/rxm7244/Projects/CoSFan-NeuralSurrogate/experiments/feedforwardmask_synthetic_continual_naive_{seed}_1.0/feedforwardmask/version_0/{pred_path}\", allow_pickle=True)[idx],\n",
    "    \n",
    "    # \"M-ER\": np.load(f\"/home/rxm7244/Projects/CoSFan-NeuralSurrogate/experiments/maml_synthetic_continual_er_{seed}_1.0/maml/version_0/{pred_path}\", allow_pickle=True)[idx],\n",
    "    # \"M-RS\": np.load(f\"/home/rxm7244/Projects/CoSFan-NeuralSurrogate/experiments/maml_synthetic_continual_task_aware_{seed}_1.0/maml/version_0/{pred_path}\", allow_pickle=True)[idx],\n",
    "    # \"M-NL\": np.load(f\"/home/rxm7244/Projects/CoSFan-NeuralSurrogate/experiments/maml_synthetic_continual_naive_{seed}_1.0/maml/version_0/{pred_path}\", allow_pickle=True)[idx],\n",
    "    \n",
    "    # \"P-ER\": np.load(f\"/home/rxm7244/Projects/CoSFan-NeuralSurrogate/experiments/pns_synthetic_continual_er_{seed}_1.0/pns/version_0/{pred_path}\", allow_pickle=True)[idx],\n",
    "    # \"P-RS\": np.load(f\"/home/rxm7244/Projects/CoSFan-NeuralSurrogate/experiments/pns_synthetic_continual_task_aware_{seed}_1.0/pns/version_0/{pred_path}\", allow_pickle=True)[idx],\n",
    "    # \"P-NL\": np.load(f\"/home/rxm7244/Projects/CoSFan-NeuralSurrogate/experiments/pns_synthetic_continual_naive_{seed}_1.0/pns/version_0/{pred_path}\", allow_pickle=True)[idx],\n",
    "}\n",
    "\n",
    "# Set up params based on test task\n",
    "param_dict = {\n",
    "    480: \"AW\",\n",
    "    475: \"DC\",\n",
    "    448: \"EC\"\n",
    "}\n",
    "\n",
    "params = get_params(\"../data/ep/\", param_dict[data[\"GT\"].shape[0]], cfg['devices'][0], 1, cfg['load_torso'], cfg['load_physics'], cfg['graph_method'])      \n",
    "\n",
    "# Example usage:\n",
    "timeframes = [0, 10, 20, 30, 40]\n",
    "visualize_potential_comparison(data, params, timeframes, figsize=(12, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
