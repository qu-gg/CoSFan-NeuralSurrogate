{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start due to the missing module '_sqlite3'. Consider installing this module.\n",
      "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresMissingModule'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import yaml\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_metrics_dict(model_dir):\n",
    "    \"\"\"Parse metrics from metrics_excel.txt file with format:\n",
    "    metric:, LP_mean(LP_std), RP_mean(RP_std), BTI_mean(BTI_std)\n",
    "    \"\"\"\n",
    "    metrics_file = f\"{model_dir}/metrics_excel.txt\"\n",
    "    \n",
    "    try:\n",
    "        with open(metrics_file, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            \n",
    "        # Initialize storage for metrics\n",
    "        metrics = {}\n",
    "        for line in lines:\n",
    "            metric, lp, rp, bti = line.strip().split(',')\n",
    "            metric = metric.replace(':', '').strip()\n",
    "            \n",
    "            # Extract mean and std from formatted strings like \"0.71290(0.10430)\"\n",
    "            def extract_mean_std(value):\n",
    "                value = value.strip()\n",
    "                mean = float(value.split('(')[0])\n",
    "                std = float(value.split('(')[1].replace(')', ''))\n",
    "                return mean, std\n",
    "            \n",
    "            lp_mean, lp_std = extract_mean_std(lp)\n",
    "            rp_mean, rp_std = extract_mean_std(rp)\n",
    "            bti_mean, bti_std = extract_mean_std(bti)\n",
    "            \n",
    "            metrics[metric] = {\n",
    "                'lp_mean': lp_mean, 'lp_std': lp_std,\n",
    "                'rp_mean': rp_mean, 'rp_std': rp_std,\n",
    "                'bti_mean': bti_mean, 'bti_std': bti_std\n",
    "            }\n",
    "        \n",
    "        return (metrics['MSE']['rp_mean'], metrics['MSE']['rp_std'],\n",
    "                metrics['SCC']['rp_mean'], metrics['SCC']['rp_std'],\n",
    "                metrics['TCC']['rp_mean'], metrics['TCC']['rp_std'],\n",
    "                metrics['MSE']['lp_mean'], metrics['MSE']['lp_std'],\n",
    "                metrics['SCC']['lp_mean'], metrics['SCC']['lp_std'],\n",
    "                metrics['TCC']['lp_mean'], metrics['TCC']['lp_std'],\n",
    "                metrics['MSE']['bti_mean'], metrics['MSE']['bti_std'],\n",
    "                metrics['SCC']['bti_mean'], metrics['SCC']['bti_std'],\n",
    "                metrics['TCC']['bti_mean'], metrics['TCC']['bti_std'])\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Error reading metrics file {metrics_file}: {str(e)}\")\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "seed = 125125125\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_highest_version(base_path):\n",
    "    \"\"\"Find the highest version folder in the given path\"\"\"\n",
    "    try:\n",
    "        # List all directories matching version_* pattern\n",
    "        version_dirs = [d for d in os.listdir(base_path) \n",
    "                       if d.startswith('version_') and 'copy' not in d.lower()]\n",
    "        if not version_dirs:\n",
    "            return None\n",
    "        \n",
    "        # Extract version numbers and find max\n",
    "        versions = [int(d.split('_')[1]) for d in version_dirs]\n",
    "        highest_version = max(versions)\n",
    "        return f\"version_{highest_version}\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error finding highest version in {base_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "metrics = {}\n",
    "base_path = \"../experiments\"\n",
    "dataset = \"synthetic\"\n",
    "\n",
    "# Define the list of seeds to process\n",
    "seeds = [1111, 2222, 3333, 4444, 5555]\n",
    "\n",
    "# Define base model paths without version\n",
    "models = {\n",
    "    \"CoMetaPNS\": {\n",
    "        # \"NL\": f\"{base_path}/feedforwardmask_{dataset}_continual_naive_SEED_1.0/feedforwardmask\",\n",
    "        \"RS\": f\"{base_path}/feedforwardmask_{dataset}_continual_task_aware_SEED_1.0/feedforwardmask\",\n",
    "        \"ER\": f\"{base_path}/feedforwardmask_{dataset}_continual_er_SEED_1.0/feedforwardmask\"\n",
    "    },\n",
    "    \"MAML-PNS\": {\n",
    "        # \"NL\": f\"{base_path}/maml_{dataset}_continual_naive_SEED_1.0/maml\",\n",
    "        \"RS\": f\"{base_path}/maml_{dataset}_continual_task_aware_SEED_1.0/maml\",\n",
    "        \"ER\": f\"{base_path}/maml_{dataset}_continual_er_SEED_1.0/maml\"\n",
    "    },\n",
    "    \"PNS\": {\n",
    "        # \"NL\": f\"{base_path}/pns_{dataset}_continual_naive_SEED_1.0/pns\",\n",
    "        \"RS\": f\"{base_path}/pns_{dataset}_continual_task_aware_SEED_1.0/pns\",\n",
    "        \"ER\": f\"{base_path}/pns_{dataset}_continual_er_SEED_1.0/pns\",\n",
    "    }\n",
    "}\n",
    "\n",
    "metrics[dataset] = dict()\n",
    "\n",
    "for model in models.keys():\n",
    "    metrics[dataset][model] = dict()\n",
    "\n",
    "    for memory in models[model].keys():\n",
    "        if memory == \"stationary\":\n",
    "            continue\n",
    "\n",
    "        # Initialize lists to store metrics across seeds\n",
    "        all_metrics = {\n",
    "            \"LP\": {\"MSE\": [], \"SCC\": [], \"TCC\": []},\n",
    "            \"RP\": {\"MSE\": [], \"SCC\": [], \"TCC\": []},\n",
    "            \"BTI\": {\"MSE\": [], \"SCC\": [], \"TCC\": []}\n",
    "        }\n",
    "\n",
    "        # Collect metrics for all seeds\n",
    "        valid_seeds = []\n",
    "        for seed in seeds:\n",
    "            try:\n",
    "                base_model_path = models[model][memory].replace(\"SEED\", str(seed))\n",
    "                # Find highest version in this path\n",
    "                version = get_highest_version(base_model_path)\n",
    "                if version is None:\n",
    "                    print(f\"No version folders found for {model} {memory} seed {seed}\")\n",
    "                    continue\n",
    "                    \n",
    "                model_path = os.path.join(base_model_path, version)\n",
    "                rp_mse_mean, rp_mse_std, \\\n",
    "                rp_scc_mean, rp_scc_std, \\\n",
    "                rp_tcc_mean, rp_tcc_std, \\\n",
    "                lp_mse_mean, lp_mse_std, \\\n",
    "                lp_scc_mean, lp_scc_std, \\\n",
    "                lp_tcc_mean, lp_tcc_std, \\\n",
    "                bti_mse_mean, bti_mse_std, \\\n",
    "                bti_scc_mean, bti_scc_std, \\\n",
    "                bti_tcc_mean, bti_tcc_std = get_metrics_dict(model_path)\n",
    "\n",
    "                # Store all metrics for this seed\n",
    "                all_metrics[\"LP\"][\"MSE\"].append((lp_mse_mean, lp_mse_std))\n",
    "                all_metrics[\"LP\"][\"SCC\"].append((lp_scc_mean, lp_scc_std))\n",
    "                all_metrics[\"LP\"][\"TCC\"].append((lp_tcc_mean, lp_tcc_std))\n",
    "                all_metrics[\"RP\"][\"MSE\"].append((rp_mse_mean, rp_mse_std))\n",
    "                all_metrics[\"RP\"][\"SCC\"].append((rp_scc_mean, rp_scc_std))\n",
    "                all_metrics[\"RP\"][\"TCC\"].append((rp_tcc_mean, rp_tcc_std))\n",
    "                all_metrics[\"BTI\"][\"MSE\"].append((bti_mse_mean, bti_mse_std))\n",
    "                all_metrics[\"BTI\"][\"SCC\"].append((bti_scc_mean, bti_scc_std))\n",
    "                all_metrics[\"BTI\"][\"TCC\"].append((bti_tcc_mean, bti_tcc_std))\n",
    "                valid_seeds.append(seed)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {model} {memory} seed {seed}: {e}\")\n",
    "                continue\n",
    "\n",
    "        if not valid_seeds:\n",
    "            print(f\"No valid seeds found for {model} {memory}\")\n",
    "            continue\n",
    "\n",
    "        # Initialize metric structure\n",
    "        metrics[dataset][model][memory] = {\n",
    "            \"LP\": {\"MSE\": {}, \"SCC\": {}, \"TCC\": {}},\n",
    "            \"RP\": {\"MSE\": {}, \"SCC\": {}, \"TCC\": {}},\n",
    "            \"BTI\": {\"MSE\": {}, \"SCC\": {}, \"TCC\": {}}\n",
    "        }\n",
    "\n",
    "        # Calculate mean and std across seeds for each metric\n",
    "        for perf_type in [\"LP\", \"RP\", \"BTI\"]:\n",
    "            for metric in [\"MSE\", \"SCC\", \"TCC\"]:\n",
    "                mean_values = np.array([x[0] for x in all_metrics[perf_type][metric]])\n",
    "                std_values = np.array([x[0] for x in all_metrics[perf_type][metric]])\n",
    "                metrics[dataset][model][memory][perf_type][metric] = {\n",
    "                    \"Mean\": np.mean(mean_values),\n",
    "                    \"Std\": np.std(std_values)\n",
    "                }\n",
    "\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "params = {'font.weight': 'bold'} \n",
    "plt.rcParams.update(params)\n",
    "\n",
    "# Define baseline values for each metric type\n",
    "baselines = {\n",
    "    \"MSE\": {\n",
    "        \"MetaPNS\": 0.00046,\n",
    "        \"FS-BO\": 0.00053,\n",
    "        \"VAE-BO\": 0.00050\n",
    "    },\n",
    "    \"SCC\": {\n",
    "        \"MetaPNS\": 0.75,\n",
    "        \"FS-BO\": 0.69,\n",
    "        \"VAE-BO\": 0.48\n",
    "    },\n",
    "    \"TCC\": {\n",
    "        \"MetaPNS\": 0.51,\n",
    "        \"FS-BO\": -1,\n",
    "        \"VAE-BO\": -1\n",
    "    }\n",
    "}\n",
    "\n",
    "# Define line styles and colors for baselines\n",
    "baseline_styles = {\n",
    "    \"FS-BO\": {\"color\": \"red\", \"linestyle\": \":\", \"linewidth\": 2},\n",
    "    \"VAE-BO\": {\"color\": \"green\", \"linestyle\": \":\", \"linewidth\": 2},\n",
    "    \"MetaPNS\": {\"color\": \"blue\", \"linestyle\": \":\", \"linewidth\": 2}\n",
    "}\n",
    "\n",
    "for metric_type in [\"MSE\", \"SCC\", \"TCC\"]:\n",
    "    dataset = \"synthetic\"\n",
    "    \n",
    "    categories = ['NL', 'RS', 'ER']\n",
    "    x = np.arange(len(categories))\n",
    "    bar_width = 0.25\n",
    "    offsets = [-bar_width, 0, bar_width]\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 3.5), dpi=500, sharex=True)\n",
    "    models = [\"PNS\", \"MAML-PNS\", \"CoMetaPNS\"]\n",
    "\n",
    "    label_font_size = 13\n",
    "    title_font_size = 16\n",
    "    legend_font_size = 12\n",
    "\n",
    "    if metric_type == \"MSE\":\n",
    "        max_val = 0.001\n",
    "    else:\n",
    "        max_val = 1.0\n",
    "\n",
    "    for i in range(3):\n",
    "        axes[i].axvspan(-0.5, 0.5, color='gray', alpha=0.5)\n",
    "        axes[i].axvspan(1.5, 2.5, color='lightblue', alpha=0.5)\n",
    "\n",
    "    # In the plotting loop, modify this section:\n",
    "    for plot_idx, perf_type in enumerate([\"LP\", \"RP\", \"BTI\"]):\n",
    "        for i, model in enumerate(models):\n",
    "            try:\n",
    "                # Get memories in the correct order based on categories \n",
    "                means = []\n",
    "                stds = []\n",
    "                for mem in ['NL', 'RS', 'ER']:\n",
    "                    mean = metrics[dataset][model][mem][perf_type][metric_type][\"Mean\"]\n",
    "                    std = metrics[dataset][model][mem][perf_type][metric_type][\"Std\"]\n",
    "                    means.append(mean)\n",
    "                    stds.append(std)\n",
    "        \n",
    "                axes[plot_idx].bar(x + offsets[i], means, width=bar_width, \n",
    "                                label=model, yerr=stds, capsize=5)\n",
    "            except Exception as e:\n",
    "                print(f\"Error plotting {model} {perf_type} {metric_type}: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "        # Add horizontal lines only for Retained Performance\n",
    "        if perf_type == \"RP\":\n",
    "            for baseline_name, value in baselines[metric_type].items():\n",
    "                line = axes[plot_idx].axhline(\n",
    "                    y=value,\n",
    "                    **baseline_styles[baseline_name],\n",
    "                    label=f\"{baseline_name}\"\n",
    "                )\n",
    "\n",
    "        direction = \"↓\" if metric_type == \"MSE\" else \"↑\"\n",
    "        titles = {\n",
    "            \"LP\": f'Learning Performance {direction}',\n",
    "            \"RP\": f'Retained Performance {direction}',\n",
    "            \"BTI\": f'Backwards-Transfer Interference ↑'\n",
    "        }\n",
    "        \n",
    "        if metric_type == \"MSE\":\n",
    "            # Set scientific notation for MSE plots\n",
    "            axes[plot_idx].ticklabel_format(style='sci', axis='y', scilimits=(0,0))\n",
    "            axes[plot_idx].yaxis.get_offset_text().set_fontsize(11)\n",
    "            axes[plot_idx].yaxis.get_offset_text().set_fontweight('bold')\n",
    "        \n",
    "        axes[plot_idx].set_title(titles[perf_type], fontsize=title_font_size, fontweight='bold')\n",
    "        \n",
    "        # Only set ylabel for leftmost subplot\n",
    "        if plot_idx == 0:\n",
    "            axes[plot_idx].set_ylabel(metric_type, fontsize=label_font_size, fontweight='bold')\n",
    "        else:\n",
    "            axes[plot_idx].set_ylabel('')  # Remove ylabel for other subplots\n",
    "            \n",
    "        axes[plot_idx].set_xticks(x)\n",
    "        axes[plot_idx].set_xticklabels(categories, fontweight='bold')\n",
    "        \n",
    "        if perf_type in [\"LP\", \"RP\"]:\n",
    "            axes[plot_idx].set_ylim(0, max_val)\n",
    "            \n",
    "        axes[plot_idx].grid(True, axis='y', alpha=0.5)\n",
    "\n",
    "    # Get bar handles and baselines manually for legend\n",
    "    bar_handles, bar_labels = axes[0].get_legend_handles_labels()\n",
    "    \n",
    "    # Add horizontal line handles manually\n",
    "    all_handles = bar_handles + [\n",
    "        plt.Line2D([0], [0], **baseline_styles[\"MetaPNS\"]),\n",
    "        plt.Line2D([0], [0], **baseline_styles[\"FS-BO\"]),\n",
    "        plt.Line2D([0], [0], **baseline_styles[\"VAE-BO\"])\n",
    "    ]\n",
    "    all_labels = bar_labels + [\"MetaPNS\", \"FS-BO\", \"VAE-BO\"]\n",
    "\n",
    "    legend = fig.legend(all_handles, all_labels,\n",
    "                       loc='upper center', \n",
    "                       bbox_to_anchor=(0.51, 0.05), \n",
    "                       ncol=6,  # Show all items in one row \n",
    "                       fontsize=legend_font_size)\n",
    "\n",
    "    legend.get_frame().set_facecolor('white')\n",
    "    legend.get_frame().set_edgecolor('black')\n",
    "    legend.get_frame().set_alpha(1)\n",
    "\n",
    "    fig.patch.set_linewidth(2)\n",
    "    fig.patch.set_edgecolor('black')\n",
    "\n",
    "    try:\n",
    "        plt.savefig(f\"continual_metric_{metric_type.lower()}_{dataset}_flattened.svg\", \n",
    "                    bbox_inches=\"tight\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving figure for {metric_type}: {str(e)}\")\n",
    "        \n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "params = {'font.weight': 'bold'} \n",
    "plt.rcParams.update(params)\n",
    "\n",
    "for metric_type in [\"DST\", \"MSE\"]:\n",
    "    for dataset in [\"mp\", \"g6_strict\"]:\n",
    "        # X-axis\n",
    "        categories = ['NL', 'T-Agnostic', 'T-Aware', 'ER']\n",
    "        x = np.arange(len(categories))  # Numeric indices for categories\n",
    "\n",
    "        # Bar width and offset to plot bars next to each other within the same category\n",
    "        bar_width = 0.2\n",
    "        offsets = [-1.5 * bar_width, -0.5 * bar_width, 0.5 * bar_width, 1.5 * bar_width]  # Adjusted for 4 models\n",
    "\n",
    "        # Create a figure with 3 subplots, sharing the same x-axis\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(13, 3.5), dpi=500, sharex=True)\n",
    "\n",
    "        models = [\"DKF\", \"VRNN\", \"RGNRes\", \"MetaRGNRes\"]\n",
    "\n",
    "        # Set a common font size for readability in a scientific paper\n",
    "        label_font_size = 13\n",
    "        title_font_size = 16\n",
    "        legend_font_size = 12\n",
    "\n",
    "        # Getting shared max ylims across LP and RP metrics\n",
    "        max_val = -np.inf\n",
    "        for model in models:\n",
    "            for mem in metrics[dataset][model].keys():\n",
    "                for met in [\"LP\", \"RP\"]:\n",
    "                    if metrics[dataset][model][mem][met][metric_type][\"Mean\"] > max_val:\n",
    "                        max_val = metrics[dataset][model][mem][met][metric_type][\"Mean\"]\n",
    "\n",
    "        if metric_type == \"DST\":\n",
    "            ylim_plus = 1\n",
    "        else:\n",
    "            ylim_plus = 0.005\n",
    "\n",
    "        # Light shading for first and last category\n",
    "        for i in range(3):  # Apply to all rows\n",
    "            axes[i].axvspan(-0.5, 0.5, color='gray', alpha=0.5)  # First category\n",
    "            axes[i].axvspan(2.5, 3.5, color='lightblue', alpha=0.5)  # Last category\n",
    "\n",
    "        \"\"\" G6 \"\"\"\n",
    "        # Plot LP\n",
    "        for i, model in enumerate(models):\n",
    "            means = np.array([metrics[dataset][model][mem][\"LP\"][metric_type][\"Mean\"] for mem in metrics[dataset][model].keys()])\n",
    "            stds = np.array([metrics[dataset][model][mem][\"LP\"][metric_type][\"Std\"] for mem in metrics[dataset][model].keys()])\n",
    "            axes[0].bar(x + offsets[i], means, width=bar_width, label=model, yerr=stds, capsize=5)\n",
    "\n",
    "        axes[0].set_title('Learning Performance ↓', fontsize=title_font_size, fontweight='bold')\n",
    "        axes[0].set_ylabel(f\"{metric_type}\", fontsize=label_font_size, fontweight='bold')\n",
    "        axes[0].set_xticks(x)\n",
    "        axes[0].set_xticklabels(categories, fontweight='bold')\n",
    "        axes[0].set_ylim(0, max_val + ylim_plus)\n",
    "        axes[0].grid(True, axis='y', alpha=0.5)\n",
    "\n",
    "\n",
    "        # Plot RP\n",
    "        for i, model in enumerate(models):\n",
    "            means = np.array([metrics[dataset][model][mem][\"RP\"][metric_type][\"Mean\"] for mem in metrics[dataset][model].keys()])\n",
    "            stds = np.array([metrics[dataset][model][mem][\"RP\"][metric_type][\"Std\"] for mem in metrics[dataset][model].keys()])\n",
    "            axes[1].bar(x + offsets[i], means, width=bar_width, label=model, yerr=stds, capsize=5)\n",
    "\n",
    "        axes[1].set_title('Retained Performance ↓', fontsize=title_font_size, fontweight='bold')\n",
    "        axes[1].set_ylabel(f\"{metric_type}\", fontsize=label_font_size, fontweight='bold')\n",
    "        axes[1].set_xticks(x)\n",
    "        axes[1].set_xticklabels(categories, fontweight='bold')\n",
    "        axes[1].set_ylim(0, max_val + ylim_plus)  # Set ylim based on max value of Accuracy and Precision\n",
    "        axes[1].grid(True, axis='y', alpha=0.5)\n",
    "\n",
    "        # Plot BTI\n",
    "        for i, model in enumerate(models):\n",
    "            means = np.array([metrics[dataset][model][mem][\"BTI\"][metric_type][\"Mean\"] for mem in metrics[dataset][model].keys()])\n",
    "            stds = np.array([metrics[dataset][model][mem][\"BTI\"][metric_type][\"Std\"] for mem in metrics[dataset][model].keys()])\n",
    "            axes[2].bar(x + offsets[i], means, width=bar_width, label=model, yerr=stds, capsize=5)\n",
    "\n",
    "        axes[2].set_title('Backwards-Transfer Interference ↑', fontsize=title_font_size, fontweight='bold')\n",
    "        axes[2].set_ylabel(f\"{metric_type}\", fontsize=label_font_size, fontweight='bold')\n",
    "        axes[2].set_xticks(x)\n",
    "        axes[2].set_xticklabels(categories, fontweight='bold')\n",
    "        axes[2].grid(True, axis='y', alpha=0.5)\n",
    "\n",
    "        # Draw bounding boxes around the two dataset columns\n",
    "        # Bounding box for 'mp'\n",
    "        # bbox_mp = plt.Rectangle((0.005, 0.00), 1.0, 0.90, edgecolor='black', fill=False, lw=2, transform=fig.transFigure)\n",
    "        # fig.patches.append(bbox_mp)\n",
    "\n",
    "        # Create a shared legend between the two columns, centered in the figure\n",
    "        handles, labels = axes[0].get_legend_handles_labels()  # Get handles and labels from one of the plots\n",
    "        legend = fig.legend(handles, labels, loc='upper center', bbox_to_anchor=(0.54, 0.05), ncol=len(models), fontsize=legend_font_size)\n",
    "\n",
    "        # Customize legend to block out lines behind it\n",
    "        legend.get_frame().set_facecolor('white')  # Set solid white background\n",
    "        legend.get_frame().set_edgecolor('black')  # Add black border around the legend\n",
    "        legend.get_frame().set_alpha(1)  # Ensure no transparency\n",
    "\n",
    "        # Adjust layout to prevent overlap and make room for the bounding boxes, titles, and shared legend\n",
    "        plt.tight_layout()  # Increased bottom space for the legend\n",
    "\n",
    "        fig.patch.set_linewidth(2)\n",
    "        fig.patch.set_edgecolor('black')\n",
    "\n",
    "        # Show the plot\n",
    "        plt.savefig(f\"continual_metric_update_nonCML_{metric_type.lower()}_{dataset}_flattened.svg\", bbox_inches=\"tight\")\n",
    "        plt.show()\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "params = {'font.weight': 'bold'} \n",
    "plt.rcParams.update(params)\n",
    "\n",
    "for metric_type in [\"DST\", \"MSE\"]:\n",
    "    for dataset in [\"mp\", \"g6_strict\"]:\n",
    "        # X-axis\n",
    "        categories = ['BGD', 'T-Agnostic RS', 'T-Aware RS']\n",
    "        x = np.arange(len(categories))  # Numeric indices for categories\n",
    "\n",
    "        # Bar width and offset to plot bars next to each other within the same category\n",
    "        bar_width = 0.25\n",
    "        offsets = [-bar_width, 0, bar_width]\n",
    "\n",
    "        # Create a figure with 3 subplots, sharing the same x-axis\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(13, 3), dpi=500, sharex=True)\n",
    "\n",
    "        models = ['MAML [1-Step]', 'MAML [5-Step]', \"Feed-Forward\"]\n",
    "        colors = ['lightblue', 'lightblue', '#d62728']\n",
    "        hatching = [\"\\\\\", \"-\", \"\"]\n",
    "\n",
    "        # Set a common font size for readability in a scientific paper\n",
    "        label_font_size = 13\n",
    "        title_font_size = 16\n",
    "        legend_font_size = 12\n",
    "\n",
    "        # Getting shared max ylims across LP and RP metrics\n",
    "        max_val = -np.inf\n",
    "        for model in models:\n",
    "            for mem in metrics[dataset][model].keys():\n",
    "                for met in [\"LP\", \"RP\"]:\n",
    "                    if metrics[dataset][model][mem][met][metric_type][\"Mean\"] > max_val:\n",
    "                        max_val = metrics[dataset][model][mem][met][metric_type][\"Mean\"]\n",
    "\n",
    "        if metric_type == \"DST\":\n",
    "            ylim_plus = 1\n",
    "        else:\n",
    "            ylim_plus = 0.005\n",
    "\n",
    "        \"\"\" G6 \"\"\"\n",
    "        # Plot LP\n",
    "        for i, (model, color, hatch) in enumerate(zip(models, colors, hatching)):\n",
    "            means = np.array([metrics[dataset][model][mem][\"LP\"][metric_type][\"Mean\"] for mem in metrics[dataset][model].keys()])\n",
    "            stds = np.array([metrics[dataset][model][mem][\"LP\"][metric_type][\"Std\"] for mem in metrics[dataset][model].keys()])\n",
    "            axes[0].bar(x + offsets[i], means, width=bar_width, label=model, yerr=stds, capsize=5, color=color, hatch=hatch)\n",
    "\n",
    "        axes[0].set_title('Learning Performance ↓', fontsize=title_font_size, fontweight='bold')\n",
    "        axes[0].set_ylabel(f\"{metric_type}\", fontsize=label_font_size, fontweight='bold')\n",
    "        axes[0].set_xticks(x)\n",
    "        axes[0].set_xticklabels(categories, fontweight='bold')\n",
    "        axes[0].set_ylim(0, max_val + ylim_plus)\n",
    "        axes[0].grid(True, axis='y', alpha=0.5)\n",
    "\n",
    "\n",
    "        # Plot RP\n",
    "        for i, (model, color, hatch) in enumerate(zip(models, colors, hatching)):\n",
    "            means = np.array([metrics[dataset][model][mem][\"RP\"][metric_type][\"Mean\"] for mem in metrics[dataset][model].keys()])\n",
    "            stds = np.array([metrics[dataset][model][mem][\"RP\"][metric_type][\"Std\"] for mem in metrics[dataset][model].keys()])\n",
    "            axes[1].bar(x + offsets[i], means, width=bar_width, label=model, yerr=stds, capsize=5, color=color, hatch=hatch)\n",
    "\n",
    "        axes[1].set_title('Retained Performance ↓', fontsize=title_font_size, fontweight='bold')\n",
    "        axes[1].set_ylabel(f\"{metric_type}\", fontsize=label_font_size, fontweight='bold')\n",
    "        axes[1].set_xticks(x)\n",
    "        axes[1].set_xticklabels(categories, fontweight='bold')\n",
    "        axes[1].set_ylim(0, max_val + ylim_plus)  # Set ylim based on max value of Accuracy and Precision\n",
    "        axes[1].grid(True, axis='y', alpha=0.5)\n",
    "\n",
    "        # Plot BTI\n",
    "        for i, (model, color, hatch) in enumerate(zip(models, colors, hatching)):\n",
    "            means = np.array([metrics[dataset][model][mem][\"BTI\"][metric_type][\"Mean\"] for mem in metrics[dataset][model].keys()])\n",
    "            stds = np.array([metrics[dataset][model][mem][\"BTI\"][metric_type][\"Std\"] for mem in metrics[dataset][model].keys()])\n",
    "            axes[2].bar(x + offsets[i], means, width=bar_width, label=model, yerr=stds, capsize=5, color=color, hatch=hatch)\n",
    "\n",
    "        axes[2].set_title('Backwards-Transfer Interference ↑', fontsize=title_font_size, fontweight='bold')\n",
    "        axes[2].set_ylabel(f\"{metric_type}\", fontsize=label_font_size, fontweight='bold')\n",
    "        axes[2].set_xticks(x)\n",
    "        axes[2].set_xticklabels(categories, fontweight='bold')\n",
    "        axes[2].grid(True, axis='y', alpha=0.5)\n",
    "\n",
    "        # Draw bounding boxes around the two dataset columns\n",
    "        # Bounding box for 'mp'\n",
    "        # bbox_mp = plt.Rectangle((0.005, 0.00), 1.0, 0.90, edgecolor='black', fill=False, lw=2, transform=fig.transFigure)\n",
    "        # fig.patches.append(bbox_mp)\n",
    "\n",
    "        # Create a shared legend between the two columns, centered in the figure\n",
    "        handles, labels = axes[0].get_legend_handles_labels()  # Get handles and labels from one of the plots\n",
    "        legend = fig.legend(handles, labels, loc='upper center', bbox_to_anchor=(0.54, 0.05), ncol=len(models), fontsize=legend_font_size)\n",
    "\n",
    "        # Customize legend to block out lines behind it\n",
    "        legend.get_frame().set_facecolor('white')  # Set solid white background\n",
    "        legend.get_frame().set_edgecolor('black')  # Add black border around the legend\n",
    "        legend.get_frame().set_alpha(1)  # Ensure no transparency\n",
    "\n",
    "        # Adjust layout to prevent overlap and make room for the bounding boxes, titles, and shared legend\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        fig.patch.set_linewidth(2)\n",
    "        fig.patch.set_edgecolor('black')\n",
    "\n",
    "        # Show the plot\n",
    "        plt.savefig(f\"continual_metric_CML_{metric_type.lower()}_{dataset}_flattened.svg\", bbox_inches=\"tight\")\n",
    "        plt.show()\n",
    "        plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
